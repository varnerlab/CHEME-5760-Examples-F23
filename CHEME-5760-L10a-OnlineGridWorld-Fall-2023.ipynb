{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "394ceef2-328a-476f-b5ea-1b8d80984b2b",
   "metadata": {},
   "source": [
    "## Example: Online Planning in the Lava Grid World\n",
    "\n",
    "This example will familiarize students with the `rollout` solution of a `two-dimensional` navigation problem, i.e., the lava world [roomba](https://www.irobot.com) problem we have discussed. \n",
    "\n",
    "### Problem\n",
    "You have a [roomba](https://www.irobot.com) that has finished cleaning the kitchen floor and needs to return to its charging station. However, between your kitchen floor and the `charging station` (safety), there are one or more `lava pits` (destruction for the [roomba](https://www.irobot.com)). This is an example of a two-dimensional grid-world navigational decision task. \n",
    "\n",
    "This example will familiarize students with using `rollout` for solving a two-dimensional grid-world navigation task, the role of the discount factor $\\gamma$. In particular, we will:\n",
    "\n",
    "* __Task 1__: Build a `5` $\\times$ `5` world model with two lava pits and a charging station.\n",
    "* __Task 2__: Generate the components of the MDP problem \n",
    "* __Task 3__: Develop on online planning solution by implementing a `rollout(...)` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f48e9efe-9565-402a-84aa-cc4f194930cd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m    Updating\u001b[22m\u001b[39m git-repo `https://github.com/varnerlab/VLDecisionsPackage.jl.git`\n",
      "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/Desktop/julia_work/CHEME-5760-Examples-F23/Project.toml`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/Desktop/julia_work/CHEME-5760-Examples-F23/Manifest.toml`\n",
      "\u001b[32m\u001b[1m  Activating\u001b[22m\u001b[39m project at `~/Desktop/julia_work/CHEME-5760-Examples-F23`\n",
      "\u001b[32m\u001b[1m    Updating\u001b[22m\u001b[39m registry at `~/.julia/registries/General.toml`\n",
      "\u001b[32m\u001b[1m    Updating\u001b[22m\u001b[39m git-repo `https://github.com/varnerlab/VLDecisionsPackage.jl.git`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/Desktop/julia_work/CHEME-5760-Examples-F23/Project.toml`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/Desktop/julia_work/CHEME-5760-Examples-F23/Manifest.toml`\n"
     ]
    }
   ],
   "source": [
    "include(\"Include.jl\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e57d202-434b-42ba-adef-0a3842a0ee4b",
   "metadata": {},
   "source": [
    "## Task 1: Build the world model\n",
    "We encoded the `rectangular grid world` using the `MyRectangularGridWorldModel` model, which we construct using a `build(...)` method. Let's setup the data for the world, setup the states, actions, rewards and then construct the world model. \n",
    "* First, set values for the `number_of_rows` and `number_of_cols` variables, the `nactions` that are avialble to the agent and the `discount factor` $\\gamma$. \n",
    "* Then, we'll compute the number of states, and setup the state set $\\mathcal{S}$ and the action set $\\mathcal{A}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "564069a4-b30f-480f-83e3-7f72a3e651f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "number_of_rows = 5\n",
    "number_of_cols = 5\n",
    "nactions = 4;\n",
    "Œ≥ = 0.25;\n",
    "nstates = (number_of_rows*number_of_cols);\n",
    "ùíÆ = range(1,stop=nstates,step=1) |> collect;\n",
    "ùíú = range(1,stop=nactions,step=1) |> collect;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20fab8b7-67fe-4b3d-9cfa-a5c863540c22",
   "metadata": {},
   "source": [
    "Next, we'll set up a description of the rewards, the `rewards::Dict{Tuple{Int,Int}, Float64}` dictionary, which maps the $(x,y)$-coordinates to a reward value. We only need to put `non-default` reward values in the reward dictionary (we'll add default values to the other locations later). Lastly, let's put the locations on the grid that are `absorbing`, meaning the charging station or lava pits in your living room:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5f17787-9756-424c-8232-7cfdf4931a00",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# setup rewards -\n",
    "rewards = Dict{Tuple{Int,Int}, Float64}()\n",
    "rewards[(2,2)] = -100000.0 # lava in the (2,2) square \n",
    "rewards[(4,4)] = -100000.0 # lava in the (4,4) square\n",
    "rewards[(3,3)] = 1000.0    # charging station square\n",
    "\n",
    "# setup set of absorbing states -\n",
    "absorbing_state_set = Set{Tuple{Int,Int}}()\n",
    "push!(absorbing_state_set, (2,2));\n",
    "push!(absorbing_state_set, (3,3));\n",
    "push!(absorbing_state_set, (4,4));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4d5829-e0bb-488c-b2c4-05d4cc6aafa6",
   "metadata": {},
   "source": [
    "Finally, we can build an instance of the `MyRectangularGridWorldModel` type, which models the grid world. We save this instance in the `world` variable\n",
    "* We must pass in the number of rows `nrows`, number of cols `ncols`, and our initial reward description in the `rewards` field into the `build(...)` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53ddd027-c480-4d63-a5b9-aa122e3aebb5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "world = VLDecisionsPackage.build(MyRectangularGridWorldModel, \n",
    "    (nrows = number_of_rows, ncols = number_of_cols, rewards = rewards));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80780e08-bac5-4b80-b73d-cc5c6dbe4bd7",
   "metadata": {},
   "source": [
    "## Task 2: Generate the components of the MDP problem\n",
    "The MDP problem requires the return function (or array) `R(s, a)`, and the transition function (or array) `T(s, s‚Ä≤, a)`. Let's construct these from our grid world model instance, starting with the reward function `R(s, a)`:\n",
    "\n",
    "### Rewards $R(s,a)$\n",
    "We'll encode the reward function as a $\\dim\\mathcal{S}\\times\\dim\\mathcal{A}$ array, which holds the reward values for being in state $s\\in\\mathcal{S}$ and taking action $a\\in\\mathcal{A}$. After initializing the `R`-array and filling it with zeros, we'll populate the non-zero values of $R(s, a)$ using nested `for` loops. During each iteration of the `outer` loop, we'll:\n",
    "* Select a state `s`, an action `a`, and a move `Œî`\n",
    "* We'll then compute the new position resulting from implementing action `a` from the current position and store this in the `new_position` variable. * If the `new_position`$\\in\\mathcal{S}$ is in our initial `rewards` dictionary (the charging station or a lava pit), we use that reward value from the `rewards` dictionary. If we are still in the world but not in a special location, we set the reward to `-1`.\n",
    "* Finally, if `new_position`$\\notin\\mathcal{S}$, i.e., the `new_position` is a space outside the grid, we set a penalty of `-50000.0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "13203c34-fd3e-4b19-8f48-825ce21d1460",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25√ó4 Matrix{Float64}:\n",
       "  -50000.0       -1.0   -50000.0       -1.0\n",
       "  -50000.0  -100000.0       -1.0       -1.0\n",
       "  -50000.0       -1.0       -1.0       -1.0\n",
       "  -50000.0       -1.0       -1.0       -1.0\n",
       "  -50000.0       -1.0       -1.0   -50000.0\n",
       "      -1.0       -1.0   -50000.0  -100000.0\n",
       "      -1.0       -1.0       -1.0       -1.0\n",
       "      -1.0     1000.0  -100000.0       -1.0\n",
       "      -1.0       -1.0       -1.0       -1.0\n",
       "      -1.0       -1.0       -1.0   -50000.0\n",
       "      -1.0       -1.0   -50000.0       -1.0\n",
       " -100000.0       -1.0       -1.0     1000.0\n",
       "      -1.0       -1.0       -1.0       -1.0\n",
       "      -1.0  -100000.0     1000.0       -1.0\n",
       "      -1.0       -1.0       -1.0   -50000.0\n",
       "      -1.0       -1.0   -50000.0       -1.0\n",
       "      -1.0       -1.0       -1.0       -1.0\n",
       "    1000.0       -1.0       -1.0  -100000.0\n",
       "      -1.0       -1.0       -1.0       -1.0\n",
       "      -1.0       -1.0  -100000.0   -50000.0\n",
       "      -1.0   -50000.0   -50000.0       -1.0\n",
       "      -1.0   -50000.0       -1.0       -1.0\n",
       "      -1.0   -50000.0       -1.0       -1.0\n",
       " -100000.0   -50000.0       -1.0       -1.0\n",
       "      -1.0   -50000.0       -1.0   -50000.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R = zeros(nstates, nactions);\n",
    "fill!(R, 0.0)\n",
    "for s ‚àà ùíÆ\n",
    "    for a ‚àà ùíú\n",
    "        \n",
    "        Œî = world.moves[a];\n",
    "        current_position = world.coordinates[s]\n",
    "        new_position =  current_position .+ Œî\n",
    "        if (haskey(world.states, new_position) == true)\n",
    "            if (haskey(rewards, new_position) == true)\n",
    "                R[s,a] = rewards[new_position];\n",
    "            else\n",
    "                R[s,a] = -1.0;\n",
    "            end\n",
    "        else\n",
    "            R[s,a] = -50000.0; # we are off the grid, big negative penalty\n",
    "        end\n",
    "    end\n",
    "end\n",
    "R"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c02d472-a0c9-494b-b356-a2eb1cdd1b4e",
   "metadata": {},
   "source": [
    "### Transition $T(s, s^{\\prime},a)$\n",
    "Next, build the transition function $T(s,s^{\\prime},a)$. We'll encode this as a $\\dim\\mathcal{S}\\times\\dim\\mathcal{S}\\times\\dim\\mathcal{A}$ [multidimension array](https://docs.julialang.org/en/v1/manual/arrays/) and populate it using nested `for` loops. \n",
    "\n",
    "* The `outer` loop we will iterate over actions. For every $a\\in\\mathcal{A}$ will get the move associated with that action and store it in the `Œî`\n",
    "* In the `inner` loop, we will iterate over states $s\\in\\mathcal{S}$. We compute a `new_position` resulting from implementing action $a$ and check if `new_position`$\\in\\mathcal{S}$. If `new_position` is in the world, and `current_position` is _not_ an `absorbing state` we set $s^{\\prime}\\leftarrow$`world.states[new_position]`, and `T[s, s‚Ä≤,  a] = 1.0`\n",
    "* However, if the `new_position` is outside of the grid (or we are jumping from an `absorbing` state), we set `T[s, s,  a] = 1.0`, i.e., the probability that we stay in `s` if we take action `a` is `1.0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b087127-75c8-40c6-b416-4ac41bd8081b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "T = Array{Float64,3}(undef, nstates, nstates, nactions);\n",
    "fill!(T, 0.0)\n",
    "for a ‚àà ùíú\n",
    "    \n",
    "    Œî = world.moves[a];\n",
    "    \n",
    "    for s ‚àà ùíÆ\n",
    "        current_position = world.coordinates[s]\n",
    "        new_position =  current_position .+ Œî\n",
    "        if (haskey(world.states, new_position) == true && \n",
    "                in(current_position, absorbing_state_set) == false)\n",
    "            s‚Ä≤ = world.states[new_position];\n",
    "            T[s, s‚Ä≤,  a] = 1.0\n",
    "        else\n",
    "            T[s, s,  a] = 1.0\n",
    "        end\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e03bd54d-4a91-4431-bddb-b932b6654452",
   "metadata": {},
   "source": [
    "Finally, we construct an instance of the `MyMDPProblemModel` which encodes the data required to solve the MDP problem.\n",
    "* We must pass the states `ùíÆ`, the actions `ùíú`, the transition matrix `T`, the reward matrix `R`, and the discount factor `Œ≥` into the `build(...)` method. We store the MDP model in the `m` variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cbe11305-9b82-4def-925c-99607990d6e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "m = VLDecisionsPackage.build(MyMDPProblemModel, \n",
    "    (ùíÆ = ùíÆ, ùíú = ùíú, T = T, R = R, Œ≥ = Œ≥));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f0aa32-4847-4ef5-999d-f403fea2ce11",
   "metadata": {},
   "source": [
    "## Task 3: Online planning solution\n",
    "First, let's set the `depth` that are going to explore, i.e., how many steps are we going to take when exploring each state `s`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "33d5d4de-39e8-4e24-a505-614413855418",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "d = 48;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b503d3-be44-4dcf-9e20-5194dbb0ef5f",
   "metadata": {},
   "source": [
    "Next, let's implement three functions:\n",
    "\n",
    "> The `myrandpolicy(problem::MyMDPProblemModel, world::MyRectangularGridWorldModel, s::Int) -> Int` function takes a `MyMDPProblemModel` instance, a `MyRectangularGridWorldModel` instance and the state `s`. This function returns a random action $a\\in\\mathcal{A}$.\n",
    "\n",
    "> The `myrandstep(problem::MyMDPProblemModel, world::MyRectangularGridWorldModel, s::Int, a::Int)` function takes a `MyMDPProblemModel` instance, a `MyRectangularGridWorldModel` instance, the state `s` and an action `a` and returns the next state $s^{\\prime}$ and reward $r$.\n",
    "\n",
    "> The `myrollout(problem::MyMDPProblemModel, world::MyRectangularGridWorldModel, s::Int64, depth::Int64) -> Float64` function takes a `MyMDPProblemModel` instance, a `MyRectangularGridWorldModel` instance, the state `s` and the depth `d`. This function returns the cumulative reward after exploring the network for `d` steps.\n",
    "\n",
    "These implementations were based on `Algorithm 9.1` of the [Decisions Book](https://algorithmsbook.com)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dc2fc656-2df9-467a-8617-a5e293c1519c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "function myrandpolicy(problem::MyMDPProblemModel, \n",
    "        world::MyRectangularGridWorldModel, s::Int)::Int\n",
    "    \n",
    "    # initialize -\n",
    "    d = Categorical([0.25,0.25,0.25,0.25]); # you specify this\n",
    "    \n",
    "    # should keep chooseing -\n",
    "    should_choose_gain = true;\n",
    "    a = -1; # default\n",
    "    while (should_choose_gain == true)\n",
    "       \n",
    "        # initialize a random categorical distribution over actions -\n",
    "        a·µ¢ = rand(d);\n",
    "        \n",
    "        # get the move, and the current location -\n",
    "        Œî = world.moves[a·µ¢];\n",
    "        current_position = world.coordinates[s]\n",
    "        new_position =  current_position .+ Œî\n",
    "        if (haskey(world.states, new_position) == true)\n",
    "            a = a·µ¢\n",
    "            should_choose_gain = false;\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return a;\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "244b7a1c-ade9-47d6-a166-91fafa73cb51",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "function myrandstep(problem::MyMDPProblemModel, \n",
    "        world::MyRectangularGridWorldModel, s::Int, a::Int)\n",
    "    \n",
    "    # get the reward value -\n",
    "    r = problem.R[s,a];\n",
    "    \n",
    "    # get the move, and the current location -\n",
    "    Œî = world.moves[a];\n",
    "    current_position = world.coordinates[s]\n",
    "    \n",
    "    # propose a new position -\n",
    "    new_position =  current_position .+ Œî\n",
    "    s‚Ä≤ = s; # default, we don't do anything\n",
    "    if (haskey(world.states, new_position) == true)\n",
    "        s‚Ä≤ = world.states[new_position];\n",
    "    end\n",
    "    \n",
    "    # return -\n",
    "    return (s‚Ä≤,r)\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "35fccd61-8532-4bfb-a345-e7dedaabc894",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "function myrollout(problem::MyMDPProblemModel, \n",
    "        world::MyRectangularGridWorldModel, s::Int64, depth::Int64)::Float64\n",
    "    \n",
    "    # initialize -\n",
    "    ret = 0.0;\n",
    "    for i ‚àà 1:depth\n",
    "        a = myrandpolicy(problem, world, s);\n",
    "        s, r = myrandstep(problem, world, s, a);\n",
    "        ret += problem.Œ≥^(i-1)*r;\n",
    "    end\n",
    "    return ret;\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b589f030-8d28-4b08-a7e1-e9a143414dee",
   "metadata": {},
   "source": [
    "Finally, we'll make a simple helper function `U(s)` that compute the value (utility) for state `s` by calling the `myrollout(...)` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ac25afca-2d8d-466a-acea-898586c6377f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "U (generic function with 1 method)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "U(s) = myrollout(m,world,s,d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e440142-f6b9-465c-9602-1550ed90a80f",
   "metadata": {},
   "source": [
    "To compute the value (utility) at each state in the network $U(s)$, we use a `for` loop:\n",
    "* For each state $s\\in\\mathcal{S}$ we call the `U(s)` helper function, which explores the problem to a depth `d`, returns the value (utility) at state `s`, and saves the value in the `utility_array`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1f8c8b82-6e21-4c94-a3ab-8cf17f77fedc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "utility_array = Array{Float64,1}();\n",
    "for s ‚àà ùíÆ\n",
    "    push!(utility_array, U(s))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7e0e4cc7-e480-403b-ae31-90f6e27d8d77",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25-element Vector{Float64}:\n",
       "      -1.3333333333388844\n",
       "    -416.3742291304142\n",
       "     -99.3699122373286\n",
       "      -1.4346602161725415\n",
       "      -1.3586644856722978\n",
       "       2.576822916644462\n",
       "     248.5352007547127\n",
       "      61.2291651145015\n",
       "      -1.333339154045879\n",
       "   -6667.932935979048\n",
       "      -1.3330937440808663\n",
       " -106250.2711694763\n",
       "     264.5575452397264\n",
       " -106642.17340596551\n",
       "      -1.7162893478378625\n",
       "      -2.8591969807942705\n",
       "       9.181369954820424\n",
       "      65.13932314973029\n",
       "  -25001.488646685404\n",
       "   -6247.360700366165\n",
       "      -1.3334326718987388\n",
       "      -1.3323190410931904\n",
       "      -1.3571749543403497\n",
       "      -1.3333334242250805\n",
       "  -25098.764031986662"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utility_array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ed50bb-90c3-479c-89b7-11a7d8dced1c",
   "metadata": {},
   "source": [
    "Extract the `action-value function` or $Q(s, a)$ from the `utility_array`. We can do this using the `Q(...)` function, which takes `m` and the `utility_array`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7bb39c49-cf77-422d-8f82-68b4d0978744",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25√ó4 Matrix{Float64}:\n",
       " -50000.3           -0.355794  -50000.3         -105.094\n",
       " -50104.1       -99937.9           -1.33333      -25.8425\n",
       " -50024.8           14.3073      -105.094         -1.35867\n",
       " -50000.4           -1.33333      -25.8425        -1.33967\n",
       " -50000.3        -1667.98          -1.35867   -50000.3\n",
       "     -1.33333       -1.33327   -49999.4       -99937.9\n",
       "     61.1338        61.1338        61.1338        61.1338\n",
       "    -25.8425      1066.14      -99937.9           -1.33333\n",
       "     -1.35867   -26661.5           14.3073     -1667.98\n",
       "     -1.33967       -1.42907       -1.33333   -51667.0\n",
       "     -0.355794      -1.7148    -50000.3       -26563.6\n",
       " -99937.9            1.29534       -1.33327     1066.14\n",
       "     65.1394        65.1394        65.1394        65.1394\n",
       "     -1.33333       -1.0625e5    1066.14          -1.42907\n",
       "  -1667.98       -1562.84      -26661.5       -50000.4\n",
       "     -1.33327       -1.33336   -50000.7            1.29534\n",
       " -26563.6           -1.33308       -1.7148        15.2848\n",
       "   1066.14          -1.33929        1.29534       -1.0625e5\n",
       "  -6251.37       -6251.37       -6251.37       -6251.37\n",
       "     -1.42907    -6275.69          -1.0625e5  -51561.8\n",
       "     -1.7148    -50000.3       -50000.3           -1.33308\n",
       "      1.29534   -50000.3           -1.33336       -1.33929\n",
       "     15.2848    -50000.3           -1.33308       -1.33333\n",
       "     -1.0625e5  -50000.3           -1.33929    -6275.69\n",
       "  -1562.84      -56274.7           -1.33333   -56274.7"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_Q = Q(m, utility_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46295cc8-cdc8-4c23-833a-3d1152480083",
   "metadata": {},
   "source": [
    "Finally, we can extract the policy $\\pi(s)$ from the action-value function $Q(s,a)$ using the `policy(...)` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "125cd5d1-c69e-4927-a0a3-46dcdf358211",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "my_œÄ = policy(my_Q);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9cfe6e55-d947-463e-8c7b-9410308bf5c8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25-element Vector{Int64}:\n",
       " 2\n",
       " 3\n",
       " 2\n",
       " 2\n",
       " 3\n",
       " 2\n",
       " 1\n",
       " 2\n",
       " 3\n",
       " 3\n",
       " 1\n",
       " 4\n",
       " 1\n",
       " 3\n",
       " 2\n",
       " 4\n",
       " 4\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 4\n",
       " 1\n",
       " 1\n",
       " 3\n",
       " 3"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_œÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a939025-1913-42e6-aa71-199c954588a1",
   "metadata": {},
   "source": [
    "### Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7b341f84-a205-45da-8356-22a2eb6f9aed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "move_arrows = Dict{Int,Any}();\n",
    "move_arrows[1] = \"‚Üê\"\n",
    "move_arrows[2] = \"‚Üí\"\n",
    "move_arrows[3] = \"‚Üì\"\n",
    "move_arrows[4] = \"‚Üë\"\n",
    "move_arrows[5] = \"‚àÖ\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "01a16373-57e1-49ae-89e5-c9f365214e1f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1) ‚Üí (2, 1)\n",
      "(1, 2) ‚Üì (1, 1)\n",
      "(1, 3) ‚Üí (2, 3)\n",
      "(1, 4) ‚Üí (2, 4)\n",
      "(1, 5) ‚Üì (1, 4)\n",
      "(2, 1) ‚Üí (3, 1)\n",
      "(2, 2) ‚àÖ\n",
      "(2, 3) ‚Üí (3, 3)\n",
      "(2, 4) ‚Üì (2, 3)\n",
      "(2, 5) ‚Üì (2, 4)\n",
      "(3, 1) ‚Üê (2, 1)\n",
      "(3, 2) ‚Üë (3, 3)\n",
      "(3, 3) ‚àÖ\n",
      "(3, 4) ‚Üì (3, 3)\n",
      "(3, 5) ‚Üí (4, 5)\n",
      "(4, 1) ‚Üë (4, 2)\n",
      "(4, 2) ‚Üë (4, 3)\n",
      "(4, 3) ‚Üê (3, 3)\n",
      "(4, 4) ‚àÖ\n",
      "(4, 5) ‚Üê (3, 5)\n",
      "(5, 1) ‚Üë (5, 2)\n",
      "(5, 2) ‚Üê (4, 2)\n",
      "(5, 3) ‚Üê (4, 3)\n",
      "(5, 4) ‚Üì (5, 3)\n",
      "(5, 5) ‚Üì (5, 4)\n"
     ]
    }
   ],
   "source": [
    "for s ‚àà ùíÆ\n",
    "    a = my_œÄ[s];\n",
    "    Œî = world.moves[a];\n",
    "    current_position = world.coordinates[s]\n",
    "    new_position =  current_position .+ Œî\n",
    "    \n",
    "    if (in(current_position, absorbing_state_set) == true)\n",
    "        println(\"$(current_position) $(move_arrows[5])\")\n",
    "    else\n",
    "        println(\"$(current_position) $(move_arrows[a]) $(new_position)\")\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495bf226-94d6-4376-a736-a650fa86db52",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.9.3",
   "language": "julia",
   "name": "julia-1.9"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.9.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
