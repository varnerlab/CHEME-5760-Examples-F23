{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "394ceef2-328a-476f-b5ea-1b8d80984b2b",
   "metadata": {},
   "source": [
    "## Example: Online Planning in the Lava Grid World"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f48e9efe-9565-402a-84aa-cc4f194930cd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m    Updating\u001b[22m\u001b[39m git-repo `https://github.com/varnerlab/VLDecisionsPackage.jl.git`\n",
      "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/Desktop/julia_work/CHEME-5760-Examples-F23/Project.toml`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/Desktop/julia_work/CHEME-5760-Examples-F23/Manifest.toml`\n",
      "\u001b[32m\u001b[1m  Activating\u001b[22m\u001b[39m project at `~/Desktop/julia_work/CHEME-5760-Examples-F23`\n",
      "\u001b[32m\u001b[1m    Updating\u001b[22m\u001b[39m registry at `~/.julia/registries/General.toml`\n",
      "\u001b[32m\u001b[1m    Updating\u001b[22m\u001b[39m git-repo `https://github.com/varnerlab/VLDecisionsPackage.jl.git`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/Desktop/julia_work/CHEME-5760-Examples-F23/Project.toml`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/Desktop/julia_work/CHEME-5760-Examples-F23/Manifest.toml`\n"
     ]
    }
   ],
   "source": [
    "include(\"Include.jl\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e57d202-434b-42ba-adef-0a3842a0ee4b",
   "metadata": {},
   "source": [
    "## Task 1: Build the world model\n",
    "We encoded the `rectangular grid world` using the `MyRectangularGridWorldModel` model, which we construct using a `build(...)` method. Let's setup the data for the world, setup the states, actions, rewards and then construct the world model. \n",
    "* First, set values for the `number_of_rows` and `number_of_cols` variables, the `nactions` that are avialble to the agent and the `discount factor` $\\gamma$. \n",
    "* Then, we'll compute the number of states, and setup the state set $\\mathcal{S}$ and the action set $\\mathcal{A}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "564069a4-b30f-480f-83e3-7f72a3e651f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "number_of_rows = 5\n",
    "number_of_cols = 5\n",
    "nactions = 4;\n",
    "Œ≥ = 0.10;\n",
    "nstates = (number_of_rows*number_of_cols);\n",
    "ùíÆ = range(1,stop=nstates,step=1) |> collect;\n",
    "ùíú = range(1,stop=nactions,step=1) |> collect;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20fab8b7-67fe-4b3d-9cfa-a5c863540c22",
   "metadata": {},
   "source": [
    "Next, we'll set up a description of the rewards, the `rewards::Dict{Tuple{Int,Int}, Float64}` dictionary, which maps the $(x,y)$-coordinates to a reward value. We only need to put `non-default` reward values in the reward dictionary (we'll add default values to the other locations later). Lastly, let's put the locations on the grid that are `absorbing`, meaning the charging station or lava pits in your living room:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5f17787-9756-424c-8232-7cfdf4931a00",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# setup rewards -\n",
    "rewards = Dict{Tuple{Int,Int}, Float64}()\n",
    "rewards[(2,2)] = -100000.0 # lava in the (2,2) square \n",
    "rewards[(4,4)] = -100000.0 # lava in the (4,4) square\n",
    "rewards[(3,3)] = 1000.0    # charging station square\n",
    "\n",
    "# setup set of absorbing states -\n",
    "absorbing_state_set = Set{Tuple{Int,Int}}()\n",
    "push!(absorbing_state_set, (2,2));\n",
    "push!(absorbing_state_set, (3,3));\n",
    "push!(absorbing_state_set, (4,4));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4d5829-e0bb-488c-b2c4-05d4cc6aafa6",
   "metadata": {},
   "source": [
    "Finally, we can build an instance of the `MyRectangularGridWorldModel` type, which models the grid world. We save this instance in the `world` variable\n",
    "* We must pass in the number of rows `nrows`, number of cols `ncols`, and our initial reward description in the `rewards` field into the `build(...)` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53ddd027-c480-4d63-a5b9-aa122e3aebb5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "world = VLDecisionsPackage.build(MyRectangularGridWorldModel, \n",
    "    (nrows = number_of_rows, ncols = number_of_cols, rewards = rewards));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80780e08-bac5-4b80-b73d-cc5c6dbe4bd7",
   "metadata": {},
   "source": [
    "## Task 2: Generate the components of the MDP problem\n",
    "The MDP problem requires the return function (or array) `R(s, a)`, and the transition function (or array) `T(s, s‚Ä≤, a)`. Let's construct these from our grid world model instance, starting with the reward function `R(s, a)`:\n",
    "\n",
    "### Rewards $R(s,a)$\n",
    "We'll encode the reward function as a $\\dim\\mathcal{S}\\times\\dim\\mathcal{A}$ array, which holds the reward values for being in state $s\\in\\mathcal{S}$ and taking action $a\\in\\mathcal{A}$. After initializing the `R`-array and filling it with zeros, we'll populate the non-zero values of $R(s, a)$ using nested `for` loops. During each iteration of the `outer` loop, we'll:\n",
    "* Select a state `s`, an action `a`, and a move `Œî`\n",
    "* We'll then compute the new position resulting from implementing action `a` from the current position and store this in the `new_position` variable. * If the `new_position`$\\in\\mathcal{S}$ is in our initial `rewards` dictionary (the charging station or a lava pit), we use that reward value from the `rewards` dictionary. If we are still in the world but not in a special location, we set the reward to `-1`.\n",
    "* Finally, if `new_position`$\\notin\\mathcal{S}$, i.e., the `new_position` is a space outside the grid, we set a penalty of `-50000.0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "13203c34-fd3e-4b19-8f48-825ce21d1460",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25√ó4 Matrix{Float64}:\n",
       "  -50000.0       -1.0   -50000.0       -1.0\n",
       "  -50000.0  -100000.0       -1.0       -1.0\n",
       "  -50000.0       -1.0       -1.0       -1.0\n",
       "  -50000.0       -1.0       -1.0       -1.0\n",
       "  -50000.0       -1.0       -1.0   -50000.0\n",
       "      -1.0       -1.0   -50000.0  -100000.0\n",
       "      -1.0       -1.0       -1.0       -1.0\n",
       "      -1.0     1000.0  -100000.0       -1.0\n",
       "      -1.0       -1.0       -1.0       -1.0\n",
       "      -1.0       -1.0       -1.0   -50000.0\n",
       "      -1.0       -1.0   -50000.0       -1.0\n",
       " -100000.0       -1.0       -1.0     1000.0\n",
       "      -1.0       -1.0       -1.0       -1.0\n",
       "      -1.0  -100000.0     1000.0       -1.0\n",
       "      -1.0       -1.0       -1.0   -50000.0\n",
       "      -1.0       -1.0   -50000.0       -1.0\n",
       "      -1.0       -1.0       -1.0       -1.0\n",
       "    1000.0       -1.0       -1.0  -100000.0\n",
       "      -1.0       -1.0       -1.0       -1.0\n",
       "      -1.0       -1.0  -100000.0   -50000.0\n",
       "      -1.0   -50000.0   -50000.0       -1.0\n",
       "      -1.0   -50000.0       -1.0       -1.0\n",
       "      -1.0   -50000.0       -1.0       -1.0\n",
       " -100000.0   -50000.0       -1.0       -1.0\n",
       "      -1.0   -50000.0       -1.0   -50000.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R = zeros(nstates, nactions);\n",
    "fill!(R, 0.0)\n",
    "for s ‚àà ùíÆ\n",
    "    for a ‚àà ùíú\n",
    "        \n",
    "        Œî = world.moves[a];\n",
    "        current_position = world.coordinates[s]\n",
    "        new_position =  current_position .+ Œî\n",
    "        if (haskey(world.states, new_position) == true)\n",
    "            if (haskey(rewards, new_position) == true)\n",
    "                R[s,a] = rewards[new_position];\n",
    "            else\n",
    "                R[s,a] = -1.0;\n",
    "            end\n",
    "        else\n",
    "            R[s,a] = -50000.0; # we are off the grid, big negative penalty\n",
    "        end\n",
    "    end\n",
    "end\n",
    "R"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c02d472-a0c9-494b-b356-a2eb1cdd1b4e",
   "metadata": {},
   "source": [
    "### Transition $T(s, s^{\\prime},a)$\n",
    "Next, build the transition function $T(s,s^{\\prime},a)$. We'll encode this as a $\\dim\\mathcal{S}\\times\\dim\\mathcal{S}\\times\\dim\\mathcal{A}$ [multidimension array](https://docs.julialang.org/en/v1/manual/arrays/) and populate it using nested `for` loops. \n",
    "\n",
    "* The `outer` loop we will iterate over actions. For every $a\\in\\mathcal{A}$ will get the move associated with that action and store it in the `Œî`\n",
    "* In the `inner` loop, we will iterate over states $s\\in\\mathcal{S}$. We compute a `new_position` resulting from implementing action $a$ and check if `new_position`$\\in\\mathcal{S}$. If `new_position` is in the world, and `current_position` is _not_ an `absorbing state` we set $s^{\\prime}\\leftarrow$`world.states[new_position]`, and `T[s, s‚Ä≤,  a] = 1.0`\n",
    "* However, if the `new_position` is outside of the grid (or we are jumping from an `absorbing` state), we set `T[s, s,  a] = 1.0`, i.e., the probability that we stay in `s` if we take action `a` is `1.0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b087127-75c8-40c6-b416-4ac41bd8081b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "T = Array{Float64,3}(undef, nstates, nstates, nactions);\n",
    "fill!(T, 0.0)\n",
    "for a ‚àà ùíú\n",
    "    \n",
    "    Œî = world.moves[a];\n",
    "    \n",
    "    for s ‚àà ùíÆ\n",
    "        current_position = world.coordinates[s]\n",
    "        new_position =  current_position .+ Œî\n",
    "        if (haskey(world.states, new_position) == true && \n",
    "                in(current_position, absorbing_state_set) == false)\n",
    "            s‚Ä≤ = world.states[new_position];\n",
    "            T[s, s‚Ä≤,  a] = 1.0\n",
    "        else\n",
    "            T[s, s,  a] = 1.0\n",
    "        end\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e03bd54d-4a91-4431-bddb-b932b6654452",
   "metadata": {},
   "source": [
    "Finally, we construct an instance of the `MyMDPProblemModel` which encodes the data required to solve the MDP problem.\n",
    "* We must pass the states `ùíÆ`, the actions `ùíú`, the transition matrix `T`, the reward matrix `R`, and the discount factor `Œ≥` into the `build(...)` method. We store the MDP model in the `m` variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cbe11305-9b82-4def-925c-99607990d6e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "m = VLDecisionsPackage.build(MyMDPProblemModel, \n",
    "    (ùíÆ = ùíÆ, ùíú = ùíú, T = T, R = R, Œ≥ = Œ≥));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f0aa32-4847-4ef5-999d-f403fea2ce11",
   "metadata": {},
   "source": [
    "## Task 3: Online planning solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "33d5d4de-39e8-4e24-a505-614413855418",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "d = 24;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dc2fc656-2df9-467a-8617-a5e293c1519c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "function myrandpolicy(problem::MyMDPProblemModel, world::MyRectangularGridWorldModel, s::Int)\n",
    "    \n",
    "    # initialize -\n",
    "    d = Categorical([0.25,0.25,0.25,0.25]); # you specify this\n",
    "    \n",
    "    # should keep chooseing -\n",
    "    should_choose_gain = true;\n",
    "    a = -1; # default\n",
    "    while (should_choose_gain == true)\n",
    "       \n",
    "        # initialize a random categorical distribution over actions -\n",
    "        a·µ¢ = rand(d);\n",
    "        \n",
    "        # get the move, and the current location -\n",
    "        Œî = world.moves[a·µ¢];\n",
    "        current_position = world.coordinates[s]\n",
    "        new_position =  current_position .+ Œî\n",
    "        if (haskey(world.states, new_position) == true)\n",
    "            a = a·µ¢\n",
    "            should_choose_gain = false;\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return a;\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "244b7a1c-ade9-47d6-a166-91fafa73cb51",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "function myrandstep(problem::MyMDPProblemModel, world::MyRectangularGridWorldModel, s::Int, a::Int)\n",
    "    \n",
    "    # get the reward value -\n",
    "    r = problem.R[s,a];\n",
    "    \n",
    "    # get the move, and the current location -\n",
    "    Œî = world.moves[a];\n",
    "    current_position = world.coordinates[s]\n",
    "    \n",
    "    # propose a new position -\n",
    "    new_position =  current_position .+ Œî\n",
    "    s‚Ä≤ = s; # default, we don't do anything\n",
    "    if (haskey(world.states, new_position) == true)\n",
    "        s‚Ä≤ = world.states[new_position];\n",
    "    end\n",
    "    \n",
    "    # return -\n",
    "    return (s‚Ä≤,r)\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "35fccd61-8532-4bfb-a345-e7dedaabc894",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "function myrollout(problem::MyMDPProblemModel, world::MyRectangularGridWorldModel, s::Int64, depth::Int64)\n",
    "    \n",
    "    # initialize -\n",
    "    ret = 0.0;\n",
    "    for i ‚àà 1:depth\n",
    "        a = myrandpolicy(problem, world, s);\n",
    "        s, r = myrandstep(problem, world, s, a);\n",
    "        ret += problem.Œ≥^(i-1)*r;\n",
    "    end\n",
    "    return ret;\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ac25afca-2d8d-466a-acea-898586c6377f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "U (generic function with 1 method)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "U(s) = myrollout(m,world,s,d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1f8c8b82-6e21-4c94-a3ab-8cf17f77fedc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "utility_array = Array{Float64,1}();\n",
    "for s ‚àà ùíÆ\n",
    "    push!(utility_array, U(s))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7bb39c49-cf77-422d-8f82-68b4d0978744",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25√ó4 Matrix{Float64}:\n",
       " -51000.2         -101.11      -51000.2           -1.12111\n",
       " -50000.1           -1.0e5      -1001.2           -1.11111\n",
       " -50000.1           98.9789        -1.12111       -1.11111\n",
       " -50000.1           -1.11111       -1.11111       -1.11111\n",
       " -50000.1           -0.110211      -1.11111   -50000.1\n",
       "  -1001.2           -1.11111   -50100.1           -1.0e5\n",
       "     -1.21112       -1.21112       -1.21112       -1.21112\n",
       "     -1.11111       -0.10011       -1.0e5         -1.11111\n",
       "     -1.11111       -2.1211        98.9789        -0.110211\n",
       "     -1.11111       -1.11111       -1.11111   -49999.1\n",
       "   -101.11          -1.11111   -50000.1       -10001.0\n",
       "     -1.0e5         -1.10101       -1.11111       -0.10011\n",
       "  -1001.1        -1001.1        -1001.1        -1001.1\n",
       "     -1.11111       -1.0001e5      -0.10011       -1.11111\n",
       "     -0.110211  -10001.0           -2.1211    -50000.1\n",
       "     -1.11111       -1.11111   -50000.1           -1.10101\n",
       " -10001.0           -1.11111       -1.11111       98.9889\n",
       "     -0.10011       -1.11111       -1.10101       -1.0001e5\n",
       "    -11.111        -11.111        -11.111        -11.111\n",
       "     -1.11111    -1001.1           -1.0001e5  -60000.0\n",
       "     -1.11111   -50000.1       -50000.1           -1.11111\n",
       "     -1.10101   -50000.1           -1.11111       -1.11111\n",
       "     98.9889    -50000.1           -1.11111       -1.11111\n",
       "     -1.0001e5  -50000.1           -1.11111    -1001.1\n",
       " -10001.0       -51000.1           -1.11111   -51000.1"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_Q = Q(m, utility_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "125cd5d1-c69e-4927-a0a3-46dcdf358211",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25-element Vector{Int64}:\n",
       " 4\n",
       " 4\n",
       " 2\n",
       " 2\n",
       " 2\n",
       " 2\n",
       " 1\n",
       " 2\n",
       " 3\n",
       " 2\n",
       " 2\n",
       " 4\n",
       " 1\n",
       " 3\n",
       " 1\n",
       " 4\n",
       " 4\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 4\n",
       " 1\n",
       " 1\n",
       " 3\n",
       " 3"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_œÄ = policy(my_Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7b341f84-a205-45da-8356-22a2eb6f9aed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "move_arrows = Dict{Int,Any}();\n",
    "move_arrows[1] = \"‚Üê\"\n",
    "move_arrows[2] = \"‚Üí\"\n",
    "move_arrows[3] = \"‚Üì\"\n",
    "move_arrows[4] = \"‚Üë\"\n",
    "move_arrows[5] = \"‚àÖ\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "01a16373-57e1-49ae-89e5-c9f365214e1f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1) ‚Üë (1, 2)\n",
      "(1, 2) ‚Üë (1, 3)\n",
      "(1, 3) ‚Üí (2, 3)\n",
      "(1, 4) ‚Üí (2, 4)\n",
      "(1, 5) ‚Üí (2, 5)\n",
      "(2, 1) ‚Üí (3, 1)\n",
      "(2, 2) ‚àÖ\n",
      "(2, 3) ‚Üí (3, 3)\n",
      "(2, 4) ‚Üì (2, 3)\n",
      "(2, 5) ‚Üí (3, 5)\n",
      "(3, 1) ‚Üí (4, 1)\n",
      "(3, 2) ‚Üë (3, 3)\n",
      "(3, 3) ‚àÖ\n",
      "(3, 4) ‚Üì (3, 3)\n",
      "(3, 5) ‚Üê (2, 5)\n",
      "(4, 1) ‚Üë (4, 2)\n",
      "(4, 2) ‚Üë (4, 3)\n",
      "(4, 3) ‚Üê (3, 3)\n",
      "(4, 4) ‚àÖ\n",
      "(4, 5) ‚Üê (3, 5)\n",
      "(5, 1) ‚Üë (5, 2)\n",
      "(5, 2) ‚Üê (4, 2)\n",
      "(5, 3) ‚Üê (4, 3)\n",
      "(5, 4) ‚Üì (5, 3)\n",
      "(5, 5) ‚Üì (5, 4)\n"
     ]
    }
   ],
   "source": [
    "for s ‚àà ùíÆ\n",
    "    a = my_œÄ[s];\n",
    "    Œî = world.moves[a];\n",
    "    current_position = world.coordinates[s]\n",
    "    new_position =  current_position .+ Œî\n",
    "    \n",
    "    if (in(current_position, absorbing_state_set) == true)\n",
    "        println(\"$(current_position) $(move_arrows[5])\")\n",
    "    else\n",
    "        println(\"$(current_position) $(move_arrows[a]) $(new_position)\")\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495bf226-94d6-4376-a736-a650fa86db52",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.9.3",
   "language": "julia",
   "name": "julia-1.9"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.9.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
